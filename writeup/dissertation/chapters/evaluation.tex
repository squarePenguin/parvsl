To evaluate the project, I worked with existing REDUCE code for regression testing
but also wrote my own tests in both plain Lisp and REDUCE RLisp code.

\section{Single-threaded building of Reduce}

My project involved modifying a large body of code and it was almost guaranteed
that I would introduce bugs during development. Since the work involves a complex,
multi-paradigm programming language, it is not possible to guarantee to cover all
possible scenarios that might exhibit new bugs. Thankfully, the language and its
direct application are well intertwined, so there is a vey large coverage test already
available to demonstrate the most functionalities of the language: building REDUCE.

REDUCE consists of around 400 thousand lines of code, and since VSL was built to
run it, all functionality in VSL is being used in REDUCE. In addition to that,
REDUCE comes with a comprehensive suite of regression tests, which were written over the
years to detect bugs in new code. Finally, almost every library in REDUCE contains a
set of tests. These tests involve a large amount of heavy computation, stress testing
many different algorithms, using large amounts of memory and requiring multiple cycles
of garbage collection.

Between building REDUCE and passing its tests, I can be confident that ParVSL retains
backwards compatibility and catch development bugs. Any error while running the code,
or any difference in output between VSL is considered a bug. This approach helped me
find most of the bugs in my code. The disadvantage was that when I had to debug there
was too much code running and it was difficult to pinpoint to origin of the problem.

To aid with this, I had multiple stages to build. The first stage was just building
the core, while the second involved building the libraries. If an error showed up while
building a library, most other libraries could be skipped to help pinpoint the problem.
The problem was not completely fixed as just building the core involved running
a very large amount of code, and libraries are also significant in size.

Most of the bugs I could have potentially introduced were data corruption bugs.
I used asserts in various places to try to get the program to crash as soon as possible
and find the issue early. Sometimes I had to introduce a system of binary searching the
problem by trapping the program early and checking for the state of the computation.
I tried to find small tests which would still reproduce the issue, then use the
\texttt{gdb} \cite{gdb} debugger to step through the code and try to find the errors. I have also used
Valgrind \cite{valgrind}, which offers a large set of tools for detecting undefined behaviour and memory
corruption.

\subsection{Bugs in VSL and REDUCE}

While debugging I found some discovered issues in VSL. The language hadn't been tested
as extensively until I started working on it. These issues were mostly minor. Some functions
were not present at all and needed to be reintroduced, for example system functions to get
working directory. There were subtle bugs such as wrong
hard-coded strings and integers. There were multiple issues with floating point manipulation,
again caused by simple human errors (e.g negated condition being checked). Switching between
output files would lose buffer content. In one place, a value on the heap was not marked
properly, meaning that if a garbage collection was triggered at the right time, it would
be collected prematurely, causing corruption. All these bugs have since been fixed.

Reduce itself showed multiple cases of sloppy code. Most of the time, the issue was using
a global symbol with a very common name liberally and then behaving weirdly
on clashes. The most striking example was when the RLisp interpreter used the symbol \texttt{x}
parsing, meaning any tests using the variable \texttt{x} would have the value be a self-reference
to code. Another instance was a name clash in the error handling function, leading to failure
to contain exceptions. While these two cases have been manually fixed, there are many other
such potential problems within the Reduce packages which will have to be cleaned up.

\subsection{Benchmarks}

In addition to helping me find issues in ParVSL, building REDUCE also provided for a
good performance benchmark. The building process simply runs Lisp and RLisp code so
running it in VSL and ParVSL will showcase the performance difference between the
two in a single-threaded case.

Figure \ref{fig:single-threaded} show the relative performance of ParVSL compared to VSL.
RCORE is the core of REDUCE, skipping non-essential packages from the build. ALG TEST is
a set of regression tests normally used to check for bugs in the code. INT TEST tests the
symbolic integration package, and provides a good performance benchmark.

\begin{figure}[h]
  \centering
  \includegraphics[width=1\linewidth]{single_threaded.png}
  \caption{Relative performance of ParVSL on single-threaded tests.}
  \label{fig:single-threaded}
\end{figure}


A slow-down between 5 and 20 percent can be observed. I have used the GProf \cite{gprof}
tool to profile the code and found the biggest cause of the time difference is the
symbol value extraction mechanism described in \ref{sec:symbols}. In VSL all symbol
values are stored globally with the symbol, whereas ParVSL adds extra book-keeping.
The symbol access function is on the critical path, and slows the code by at least
five percent. This penalty would easily be eliminated in a compiled language, where the
checks for global and fluid variables could be performed statically and optimised away
from the runtime. Other causes of the slowdown are thread-local variables in the interpreter,
which I discuss in section \ref{sec:crossplatform}.

\section{Multi-threading unit tests}

While REDUCE provides a large suite of tests for single-threaded behaviour, I had to come
up with new tests for multi-threading. Before going into larger examples, I started with a small
suite of unit tests in Lisp.

The following test shows multiple threads sharing a variable. They have to acquire
a mutex to prevent a data race. All they do is increment it by one, so the final
value should be equal to the number of calls.

\label{lst:shared-global}
\begin{verbatim}
(global '(x x_mutex))
(setq x 0)
(setq x_mutex (mutex))

(de incr_x ()
  (mutex_lock x_mutex)
  (setq x (add1 x))
  (mutex_unlock x_mutex))

(dotimes (i 10000) (thread '(incr_x)))
(print x) % 10000
\end{verbatim}

When modifying the garbage collector, I had issues when multiple
threads were initiating it, for example I discovered a deadlock
with my original GC locks (section \ref{sec:gclock}), which I then
quickly fixed. It was easily reproduced with the following test:

\label{lst:multi-gc}
\begin{verbatim}
% reclaim forces garbage collection
(dotimes (i 16) (thread '(reclaim)))
\end{verbatim}

Big numbers use continuous areas of memory and can easily
fill in entire segments all at ones. The code below simply
raises a running counter to itself in parallel. It helped me discover
a that my code was not handling an allocation request being
larger than the segment size.

\label{lst:number-crunch}
\begin{verbatim}
% a naive recursive power function
(de pow (a b)
  (cond
    ((zerop b) 1)                  % if b = 0: return 1
    (t (times a (pow a (sub1 b)))) % else: return a * pow(a, b - 1)
  ))

(dotimes (i 1000)
  % raise i^i for really large numbers
  (thread 'pow (list i i)))
\end{verbatim}

These tests, among others, were very helpful in finding bugs in the original
during development. They were added when testing new functionality,
sometimes as a result of finding a bug in a larger example.
They now act as regression tests for ParVSL.

\section{Thread pool}

Once I showed  that ParVSL could run both single-thread code (i.e build REDUCE) and pass some
simple tests for multi-threading, I was able to write more complex code using threads.

Spawning hardware threads directly to parallelise each task can be undesirable. The user has
to manage the lifecycle of each thread, making sure to join it and also to manage the number
of available threads on the current hardware directly. Failure to do so will quickly result in
over-subscription of threads. Each thread object comes with its own overhead including a local
stack and operating system handle,

A thread pool is a structure for simplifying parallelism by abstracting away the interaction
with hardware threads. A thread pool consists of a work queue for pending jobs and a number
of worker threads which execute those jobs as they become available. The number of workers
can equal the number of hardware threads so that the program never has to spawn more threads
than there available, and threads can be reused. Once a thread pool is created, the user simply
needs to submit jobs and they will be automatically parallelised.

\subsection{A thread-safe queue}

The main data structure behind the thread-pool is a thread-safe queue. All threads may push
jobs to this queue and all working threads pop tasks from it to execute. Jobs can be executed
in any order, and I a queue so that they would be executed in the order they are submitted, which
seemed the most natural.

We can implement such a queue easily in ParVSL using a mutex and a condition variable. We start
from a simple queue, with the following functions:

\begin{itemize}
\item \texttt{queue()} creates a new queue
\item \texttt{queue\_push(q, x)} pushes value \texttt{x} to queue \texttt{q}
\item \texttt{queue\_pop(q)} pops and returns the value at the front of the queue
\item \texttt{queue\_empty(q)} checks if the queue is empty
\end{itemize}

A thread\(_{\text{safe}}\) queue is simply a wrapper on top of the normal queue:
\begin{verbatim}
procedure safe_queue();
  {queue(), mutex(), condvar()};
\end{verbatim}

We need two procedures: \texttt{safe\_queue\_push(sq, x)} and \texttt{safe\_queue\_pop(sq)}. The latter will
wait if the queue is empty until an element is enqueued. The waiting is done using the condition variable:

\begin{verbatim}
procedure safe_queue_pop(sq);
begin
  scalar q, m, cv, res;
  % unpack the safe queue
  q := first sq;
  m := second sq;
  cv := third sq;

  mutex_lock m;

  while queue_empty q do
    % wait for another thread to push an element and notify
    condvar_wait(cv, m);

  res := queue_pop q;
  mutex_unlock m;
  return res;
end;
\end{verbatim}

Now, the push method must notify the condition variable if the queue was empty.
\begin{verbatim}
procedure safe_queue_push(sq, x);
begin
  scalar q, m, cv;
  q := first sq;
  m := second sq;
  cv := third sq;

  mutex_lock m;
  queue_push(q, x);
  condvar_notify_one cv;
  mutex_unlock m;
end;
\end{verbatim}


\subsection{Managing threads}

With the queue implemented we can design the worker threads. The starting thread
initialises the queue and starts all the workers as individual threads. It can start
either the maximum number of hardware threads (which can be determined using the
\texttt{hardware\_threads()} function), or a custom count. Each thread is passed a reference to
the thread pool, so it can access the queue. Once the threads are started, they will only
be joined on exit or when the user manually stops the pool.

The mechanism for stopping the queue is a simple atomic flag. Atomics are not offered as
a primitive in ParVSL, but can be easily implemented with a mutex lock. There is no direct
mechanism for interrupting a thread running a task, but workers can check the flag every time
before taking a new task from the queue.

Using the blocking call to wait for the next job mean the worker get stuck and prevent
stopping the thread-pool. When the the user tries to stop an empty pool, then all
the workers will be in a sleeping state, waiting for the queue condition variable to be
notified, causing a deadlock.

\begin{verbatim}
while atomic_get(run_flag) = 'run do
  // The workers can get stuck here waiting on an empty queue
  job := safe_queue_pop(sq)
  run_job job;
\end{verbatim}

Another idea is to not use a blocking call to pop from the queue, but rather spin:
\begin{verbatim}
while atomic_get(run_flag) = 'run do
  job := safe_queue_try_pop(sq)
  if job then
    // trypop succeeded
    run_job job
  else
    // important to yield here
    thread_yield()
\end{verbatim}

This approach solves the issue, but it is important to note the \texttt{thread\_yield()} call.
I have implemented \texttt{thread\_yield()} to directly call the C++ equivalent. This allows the
system to schedule other threads, making sure a waiting worker does not spin the CPU
core to 100\% until forcefully preempted by the OS. It would also prevent other threads
started by the user from doing work.

\subsection{Waiting for a job's result}

In ParVSL, the \texttt{thread} function takes another function to execute on the new thread,
along with the arguments for that function. The return value of the function call is
then recovered when joining the thread with \texttt{join\_thread}, enabling thread communication.

When switching from threads to jobs in the thread pool, we want to maintain this functionality,
otherwise the only way to communicate between parallel jobs would be through global state,
which would severely limit its usefulness. Passing argument for a job is trivial, as they are simply
stored in the safe queue, along with the function to be called. However, returning the result of
a job required extra book-keeping.

Using the primitives in ParVSL, we can implement a \texttt{future} type. A future is a helpful mechanism
that allows us to both wait for a task and obtain its return value. A future starts out as empty.
It can have any number of readers but only one writer. The writer is usually the creator of the future
and will set its value exactly once, at some point after creation. The readers can then try to get the
value inside the future. If the future is fulfilled, the get call returns instantly. Otherwise, it
becomes a blocking call, waiting until the future is set, then returning the respective value.

Implementing a future is similar to the safe queue, using a mutex and a condition variable.
Getting and setting the future requires acquiring the lock. The getter has to wait on the condition
variable if the future is not set. The setter notifies all the getters after setting the value.
The full implementation can be found in the Appendix.

With the future implemented, we can finish the thread pool, having a mechanism for pushing a job:

\begin{verbatim}
procedure thread_pool_add_job(tp, fn, args):
  fut := future()
  safe_queue_push(tp.safe_queue, {fn, args, fut})
  return fut
\end{verbatim}

The caller can use the future to wait for the result of the job and the workers need to set the future
when finishing a job. Finally, I note that the thread pool must deal with exception handling. The worker threads
need to catch any error while running the job and report it through the future. Initially, I failed to include
it meaning that worker threads unwounded unsafely. This lead to the the thread-pool being unable to signal the
thread and fail to terminate. Additionally, threads waiting for the result would also be stuck.

There are many aspects to be considered in the design of a thread pool. I have focused on the main ones, and
this thread pool was sufficient for the rest of evaluation. I have successfully used to to parallelise the
other experiments in this report. However, depending on the task it could be improved upon with more features.
Currently, the number of threads is static, but it could dynamically start and stop threads to accommodate
the workload. A more efficient safe queue could be implemented using granular locking. Furthermore, we could
reduce contention on the queue by having each worker keep its own queue, and the main queue would act as a
dispatcher.

\section{Implementing Parallel Mergesort}

To test the correctness and performance of ParVSL I implemented a few classic algorithms that are relatively
easy to parallelise. Sorting is a particularly good example. Mergesort splits a list in two, sort each half
recursively, then merges the results to obtain the sorted list. Sorting the individual halves can be done
in parallel.

\begin{verbatim}
tp := thread_pool()

procedure parallel_merge_sort(list):
  if length(list) < 2:
    return list

  xs, ys := split(list)
  sorted_xs_future := thread_pool_add_job(tp, 'parallel_merge_sort, {xs})
  sorted_ys := parallel_merge_sort(ys)
  sorted_xs := future_get(sorted_xs_future)

  return merge(sorted_xs, sorted_ys)
\end{verbatim}

We use the thread pool implemented above to achieve parallelism. Without the thread pool, we would
have to manually manage threads. Using threads here would have resulting in a new thread spawned for
each element in the array. The function would already oversubscribe threads for lists as small as
100 elements. The thread pool only uses a constant number of threads.

\subsection{Dealing with tasks waiting for other tasks}

However, the naive implementation above is incorrect and it will deadlock as soon as the number of
jobs exceeds the number of workers. This highlights a shortcoming of the thread pool. In its current
state is does not handle tasks enqueuing and then waiting for other tasks. In this case, all the workers
will end up waiting for the future (\texttt{sorted\_xs\_future}) without doing any work.

To fix this, I have added extra functionality to the thread pool. An extra procedure \texttt{thread\_pool\_run\_job}
allows can be called by any thread to run on job on the queue. This procedure is implemented similarly
to the worker function, except it only takes at most one job (or none if the queue is empty) from the
queue instead of looping.
This function should be called by any job which is waiting for another job in the thread pool.

I also needed to implement another function for futures \texttt{future\_try\_get}, which only returns the
value in the future if it was fulfilled, without blocking, or indicates failure, without blocking.

Subsequently, I have changed line \texttt{10} above to the following code:
\begin{verbatim}
while null (future_try_get(sorted_xs_future):
  thread_pool_run_job(tp)
\end{verbatim}
Now, workers can start (and finish) other jobs while waiting and will not deadlock.

% \section{{\bfseries\sffamily TODO} Results}
\subsection{Results}

To test the correctness, I simply generate a list of random numbers, then compare the output
to that of the sorting function built in Reduce. Afterwards, I could test for performance.
I first tuned the parallel version to use the sequential algorithm once if the list is too
small. I found on my machine that around parallelisation became useful once the size of
the list was larger than 1000, and I tuned it to a threshold of 5000. Without this optimisation
the parallel version would spawn too many jobs (\(O(N)\) to be more exact) and the time book-keeping
would completely eliminate any benefit of multi-threading. Indeed, it runs an order of magnitude
slower on large lists (over 1000 elements).

I have plotted the speed-up boost offered for a number of array lengths, depending on the number
of workers.


\begin{table}
\centering
\begin{tabular}{rrrrr}
 & 100000 & 250000 & 500000 & 1000000\\
\hline
1 & 2.351 & 5.282 & 11.152 & 22.840\\
2 & 1.563 & 3.409 & 5.850 & 11.706\\
8 & 1.319 & 2.554 & 4.906 & 9.054\\
16 & 1.071 & 2.137 & 3.995 & 7.514\\
\end{tabular}
\caption{Parallel merge sort times by number of workers}
\label{table:parmergesort}
\end{table}

\section{Parallel building of Reduce}

The entirety of REDUCE is made up of RLISP code, which VSL and ParVSL
can simply run to build it. The process can be separated into two steps:
building the core of REDUCE, and then building all the additional packages.
The core, which I will refer to as RCORE, satisfies most of the dependencies
any of the additional packages need, and only takes a fraction of the time
to build.

\begin{center}
\begin{tabular}{lr}
 & Time (s)\\
\hline
RCORE & 11.83s\\
REDUCE & 59:93s\\
\end{tabular}
\end{center}

Once good use of multi-threading would be to speed up this build time by
arranging to build individual REDUCE packages in parallel. The RCORE base
should still be built sequentially, as it is much more difficult to
separate independent tasks. However, the rest of the packages mostly
only have RCORE as a dependency.

I extracted a list of 64 packages to build. The starting point was the
RCORE image. On VSL, I simply ran the building sequentially:

\begin{verbatim}
for package_name in packages do
  build_package package_name;
\end{verbatim}

The total running time was \texttt{14.190}. Then, I used the thread pool
to run these builds in parallel:

\begin{verbatim}
tp := thread_pool(hardware_threads());

pack_futures := {};
for package_name in packages do <<
  pack_future := thread_pool_add_job 'build_package {package_name};
  pack_futures := pack_future . pack_futures;
>>;

% We need to ensure all jobs are finished.
for pack_future in pack_futures do
  if (future_get pack_future) != nil do
    print "error building package";
\end{verbatim}

Running this code will initially fail building any package. This is caused
by global side-effects of building the packages. The first issue is that all
packages use global symbols with common names for storage. Any global
symbol used by a package should use a globally unique name, usually by prepending
the package name to the symbol name. Unfortunately, the writers of these packages
didn't always follow good practices. It is outside the scope of this project to
fix all Reduce packages, however I tried to work my way around that.

I modified the ParVSL code so that when I enable a compilation flag it tracks all
reads and writes to global values of fluid or global symbols. Then, I wrote a python
script which builds each package individually with the build flag on and saved those
accessed to a file. The script then computed all conflicts between packages. The script
then generated a Reduce test file which would remove all global access on building the packages:

\begin{verbatim}
% Force conflicting symbols to be fluid.
% Here, global1, global2, etc. are the names of those conflicting symbols
fluid '(global1 global2 ...)
fluid '(store!-global1 store!-global2 ...)

store!-global1 := global1;
store!-global2 := global2;
...

procedure build_package_safe(name);
begin
  % we bind the conflicting variables locally
  scalar global1, global2 ...;

  % restore their original global values inside the local scope
  global1 := store!-global1;
  global2 := store!-global2;
  ...

  build_package(name);
end;
\end{verbatim}

The generated file makes use of the dynamic scoping mechanism in Lisp. When building the package
with \texttt{build\_package\_safe}, all accesses to those global symbols will instead access locally
bound ones. This removes all conflicts between symbol accesses, without needing to modify code in
the Reduce packages themselves.

The second issue I found was that any package can change the access specifier of a global symbol.
One package could make a symbol global, while another tries to bind it locally causing an error.
This issue could not be solve at the Lisp level, so I had to add another temporary flag to the
ParVSL interpreter. Effectively, I disabled globals altogether, and made every global symbol
fluid instead. Fluids allow all the functionality of globals, plus they will allow other threads
to locally bind the name.

Finally, I discovered an inherent design flaw in the Lisp language used by Reduce which I could not
fix. A feature in Reduce allows the user to set flags to a symbol. These flags always affect the global
symbol. Many Reduce packages use this feature, adding, retrieving and removing flags from shared
global symbol names. I could not find a way to around this issue without modifying all of Reduce.
This meant I was not able to parallelise the building of most Reduce packages.

Luckily, I did manage to find at least 20 packages which would not conflict in modifying symbol flags.
I parallelised the building of those, the built the other packages sequentially. The resulting
running time was \texttt{8.771s}. This showed a 40\% improvement, which is very promising.
I believe that fixing the issue of shared global names in Reduce could lead to a big improvement
of build time when using multi-threading.

\section{Testing ParVSL on different platforms}
\label{sec:crossplatform}

My benchmarks so far have focused on x86 Linux, running
up-to-date compilers. VSL is a cross-platform language which should run on any system as long as the
C++ compiler supports it. ParVSL further requires C++11 support, however at the
time of this writing, the standard is already widely supported on all major platforms.
I have also ran my test on different systems to verify this claim.

I tested three major operating systems: Linux, MacOS, and Windows. I also
ran the tests on a Raspberry PI to test it on the ARM platform. Compiling on the Raspberry PI
posed problems even for the single-threaded VSL, as only older compilers are available
there and they exhibiting some bugs in compiling the code and complained about valid code.
The other systems successfully compiled and ran all the code.

The more surprising result is the vast difference in performance between the platforms
when running ParVSL vs VSL. The following are times

\begin{center}
  \begin{tabular}{lrr}
                       & VSL    & ParVSL \\
  \hline
  x86 Linux            &   58s & 1m08s \\
  x86 MacOS            & 1m19s & 3m56s  \\
  x86 Windows (Cygwin) &   1ms & 10m15 \\
  \end{tabular}
  % \caption{Building Reduce on different platforms VSL vs ParVSL}
\end{center}


\subsection{Thread-local access performance}

The biggest culprit for the performance imact is the system's thread local storage (TLS)
mechanism.

\begin{figure}
\centering
\includegraphics[width=1\linewidth]{thread_local.png}
\label{fig:thread-local}
\end{figure}

As we can observe, simply using thread-local storage can have a big impact on performance.