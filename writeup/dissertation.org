# #+TITLE: Implementing parallelism in Lisp for REDUCE
# #+SUBTITLE: Computer Science Part II Project
# #+AUTHOR: Andrei Vlad Badelita
# #+EMAIL: avb40@cam.ac.uk

#+OPTIONS: toc:nil
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, 12pt]
#+LATEX_HEADER: \usepackage[margin=3cm]{geometry}
# #+LATEX_HEADER_EXTRA:
#+DATE: \today

#+BEGIN_EXPORT latex
% Title

\pagestyle{empty}

\hfill{\LARGE \bf Andrei-Vlad Bădeliță}

\vspace*{60mm}
\begin{center}
\Huge{\bf Implementing Parallelism in Lisp for REDUCE} \\
\vspace*{5mm}
Part II Computer Science Dissertation \\
\vspace*{5mm}
Trinity College \\
\vspace*{5mm}
\today  % today's date
\end{center}

\cleardoublepage
#+END_EXPORT

#+LATEX: \newpage

\section*{Declaration of originality}

I, Andrei-Vlad Bădeliță of Trinity College,
being a candidate for Part II of the Computer Science Tripos,
hereby declare that this dissertation and the work described in it
are my own work, unaided except as may be specified below, and
that the dissertation does not contain material that has already
been used to any substantial extent for a comparable purpose.

Signed 

Date

\section*{Proforma}

TODO Write proforma
- Your candidate number.
- The Title of your Project.
- The Examination and Year.
- Word-count for the dissertation.
- Final line count: Number of lines written by the *student* in the final version of their software work.
- Project Originator.
- Project Supervisor.
- At most 100 words describing the original aims of the project.
- At most 100 words summarising the work completed.
- At most 100 words describing any special difficulties that you faced. 


#+LATEX: \newpage

#+TOC: headlines 2

* Introduction
The motivation for this project is to explore the implementation of multi-threading
capabilities within a working compiler and assess the benefits and tradeoffs it brings
to a real-world application with a large, actively-developed body of code. 

** REDUCE

[[https://reduce-algebra.sourceforge.io/][REDUCE]] is a portable general-purpose algebra system (CAS). It enables symbolic
manipulation of mathematical expressions and provides a wide range of algorithms
to solve problems spanning many fields, with arbitrary precision numerical approximation. 
It has a friendly user interface and can display maths and generate graphics.

REDUCE is one of a few open-source general-purpose CAS programs, alongside Maxima and Axiom.
The three projects are all built on top of different Lisp kernels. At the time of this writing,
none of these projects have any multi-threaded capabilities. My aim is to remove this limitation
for REDUCE. The project is using its own Lisp dialect which is similar to Common Lisp, but has its
own design and set of capabilities.

** VSL

Over the years, there have been multiple implementations of the Lisp compiler REDUCE is using, with
different goals: PSL, CSL and VSL. VSL is an interpreted language written in the programming
language ~C~. It is fully capable of building the entirety of REDUCE, supports all the major 
platforms and architectures, and is well optimised for speed, minimising the performance tradeoff
of being interpreted.

** Benefits of multithreading

The idea of using parallel computing to speed up computer algebra computing has come
up in research papers for many years ~[TODO reference it]~, but much of the activity now
predates the now ubiquitous multi-core CPUs used in modern computers and the amount of memory
they now provide. Moreover, advancements in single-core CPU performance has slowed down
significantly, as clock speeds have stagnated and even gone down in recent years. The biggest
area of imrpovement in these new CPUs is their core count and number of hardware threads.
Binding the performance of Reduce to single-threaded performance is likely to lead to
a limitation in speed gains from new hardware.

** Code examples

To help explain the concepts I introduce, I will use code fragments showing the algorithms.
These fragments will be a simplification of the original code, in order to remove the need
for context within the rest of the codebase. I will use two programming languages in this review:
C++ and OCaml. C++ code will be used where the original code is on the Lisp interpreter.

For concepts related to the target language, I will be using OCaml instead of Lisp for readability.
Just like the original Lisp was only intended as an intermediate representation, Lisp is not used
in REDUCE directly. An additional language called ~RLISP~ is used instead. RLISP is simply aiming
to provide a more readable syntax, however this syntax is ALGOL inspired and unfamiliar to most
programmers nowadays. I have decided to use an ML style syntax in this writeup, which should aid
with brevity and readability.

* Preparation
** ParVSL
I have forked the the original VSL project into a new language which I call for simplicity Parallel VSL, or ParVSL.
ParVSL is fully backwards compatible with VSL and will be tested against it for performance.

VSL was a good candidate for this project because it featured a complete, working Lisp implementation while being
small enough to be a managable project. The entire VSL codebase consists of around 10000 lines of code, all of which
was originally contained within a single file. Before making changes in critical areas, I spent some time familiarising
myself with the code. This included spliting the project in more more files, fixing a few obvious bugs, and porting
to C++.

** From C to C++

The VSL language was written in the programming language C. C is a language with no standardised
multi-threaded model and no native support for multi-core programming. Furthermore, it has no well-defined
memory model, and no defined ordering of memory accesses. Multi-threaded programming
is only possible in C through a third-party library, such as ~Windows threads~ or the 
the ~POSIX threads~ library on UNIX systems. Support further has to be guaranteed by individual compilers
and operating systems and can break between versions.

C++ is a superset of C and can compile existing, standard C code easily. The C++11 standard addresses the
above omissions, making C++ a multi-threaded language. While in some cases the implementation uses the same
libraries as the C equivalend (e.g POSIX), we do not have to think about these details and the code
we write is fully portable. The only requirement is that a C++11 compliant compiler is used to compile the
code and then which platforms these compilers can target. As of today, the C++11 standard has matured enough
that all the large compiler vendors (i.e GCC, Clang, Visual C++, Mingw, etc.) fully support it on the
major platforms (e.g x86, ARM and SPARC). 

The first change I have made to the implementation is to clear it of any incompatible code and compile it
with a C++ compiler. This was a trivial task and mostly involved adding a few more explicit casts.
However, I have been slowly transitioning the code from idiomatic C++ as I analysed more parts of it
and became confident those changes wouldn't affect the semantics of the program.

** Throughput vs latency
When optimising for performance in a programming language, we have to analyse the tradeoff between
total thoughput and latency. Optimising for latency means minimising the duration of any individual
task in the program, and increasing availability. Optimising for throughput involves minimising the
total running time of the program. For example, a web server would benefit more from reducing latency
of any individual request.

In a CAS program the user is most likely to care about throughput, i.e. compute the output of large 
problem sizes as quickly as possible. The program is single-user and has a simple interface. The only case
for low latency is in the responsiveness of the graphical user interface. This is already provided by the 
operating system so our main goal is directed towards minimising throughput in the application. 
This is particularly important when designing the [[Garbage collection]].

** Memory allocation
Memory is managed by the interpreter. It allocates a large block of memory at the beginning,
which it then manages as a contiguous array. When running out of memory, an extra block of the
same size as current available memory is malloced, doubling the amount available. These blocks are
never freed until the end of the program. They are sorted by their pointer locations,
and carefully /joined/ together to maintain the abstract model of continguous memory. Binary search
is used to identify the block containing a location.

** Garbage collector
An important feature of Lisp languages is their garbage collectors. Garbage collectors allow the programmer
to design code without having to worry about the lifecycle of their data, the internal memory model and
managing pointers. This makes Lisp code significantly easier to write, leaving the burden of providing safety and
efficiency to the compiler.

In effect, the garbage collector is an important component of the VSL interpreter and careful considerations
have to be made when modifying it. First of all, any bugs in the garbage collector may leave the memory in 
an invalid state, corrupting the state of the program and leading to undefined behaviour in C. Such errors 
are also very difficult to spot and debug, as they can go undetected until the particular region of memory
is accessed again.

*** Cheaney's algorithm
The approach a garbage collector uses to deal with freed memory affects both its performance and memory usage.
Before the first garbage collection cycle, memory can simply be allocated in a continuous fashion, making it
compact and fast. When the garbage collector finds unreachable objects and eliminates them, they will leave /gaps/ behind
and cause /fragmentation/. Not dealing with fragmentation leads to wasted memory. Keeping track of the gaps
and filling them with objects of the right size involves extra book-keeping which can be quite expensive. Ultimately,
it is impossible to guarantee the gaps are filled efficiently, because the garbage collector cannot predict future
memory allocations, and thus heuristics have to be employed.

VSL avoids this problem entirely by using a copying garbage collector. This means it compacts memory by moving 
traceable objects to a new region. The unreachable objects are simply not moved and they will eventually be overwritten. 
This method has the advantage that it fully compacts memory, fixing the issue of fragmentation in an efficient,
straight-forward way. The main trade-off this approach has is the total memory usage. A region of memory at least
as large as the one in use has to be used to copy the live objects into. In addition, when a very large amount 
of memory is in use, the copying of all live memory might become more expensive than managing the free memory.
Finally, this approach is not incremental. The entire heap is scanned and cleaned every run. Other ways might
reduce latency by collecting only partially in more, but shorter cycles. 

The problem sizes in REDUCE are usually not bound by memory on modern computers, with none of the applications
being known to use more than one gigabyte. Its application is also not latency-sensitive. It is used for
solving large problems as fast as possible. A large compaction stage is more efficient as it reduces time 
switching between running code and collecting garbage and minimises fragmentation.
This makes the copying approach suitable for the language. 

Cheaney's algorithm is a method of stop-the-world copying garbage collection. The virtual /heap/ is divided into
two halves, and only one half is in use for allocations. The other half is considered free and used during garbage
collection. When the first half is full and garbage collection is triggered, all traceable objects are copied over
to the second half. Then, the two halves are swapped.

To start the tracing we need to consider a root set: a subset of object which are known to be accessible.
One example of elements in the root set is the table of symbols which are defined at the start
of the garbage collection. The stack will also contain pointers to objects and must be scanned when computing the root set.
While these are the main compontents of the root set, the interpreter may contain others depending on language features
and implementation, all of which must be spotted and added. The root set must be conservative as missing any object which
should be in the root set will result in collection of live objects, invalidating the program. It is always
preferable to overapproximate the root set.

Once we have distinguished a root set, we can trace all references to build the reachable set. Objects may contain references
to other objects, which are also considered reachable. For example, all elements of a list must be traced recursively. 
The reachable set is the transitive closure of the reachability relation. All objects in the reachable set must be kept
during collection, while eveything else may be safely collected.

The algorithm presented above is a heavily simplified abstraction. The root set includes other
locations apart from the symbol table, and all of them have to be handled carefully. This was an area
of particular importance when adding multi-threading.

The ~copy~ and ~copycontent~ procedures have to read the heap to determine the type, size and fields of
the ~LispObject~, and act accordingly. This approach of storing all the information inside the virtual heap
and manually accessing it as needed has significant performance benefits. ~LispObject~ becomes a simple aliasing
for a pointer type, it allows many different types of objects (integers, floats, strings, lists) to be accessed
in a unified way, while staying compact in memory. The disadvantage is that it is is difficult to track memory
corruption, making debugging more difficult. This will become a problem if any bug in the code produces a data-race.

Please refer to the [[Appendix]] for a more detailed implementation of the algorithm.

**** TODO reference cheney
*** Conservative GC
One important design aspect of calculating the root set is how to handle references on the stack. Garbage collection
may start in the middle of a large computation and the references on the stack cannot be discarded. One safe, but slow
approach of dealing with this is to keep a virtual stack. Such a stack could be well typed and easily scanned to find
references. However, it would be much slower by adding a level of indirection to each expression, and it would also make
the code more difficult to manage.

Another approach is to tag words in memory. This approach is used, for instance, by the OCaml compiler, where the least
significant bit is a tag bit, indicating whether that word is a pointer reference or just data. This approach is better
than the virtual stack, but has the drawback of limiting the integer types (e.g 63bit vs 64), and requiring additional
instructions (i.e shifts) to do arithmetic.

The approach VSL uses is to be conservative. It treats all values on the stack as potential references, called /ambiguous/ roots.
This means we are overestimating the set of roots. Unlike /unambiguous/ roots (like the symbol table above), we
have to be careful when handling these values, and cannot manipulate them as ~LispObject~. This rules out calling
~copy~ or ~copycontent~ above on them, but they still need to be kept during garbage collection. The solution was
to /pin/ them. Any location on our heap which is pointed to by an ambiguous root is pinned and not copied over.
Additionally, the ~allocate~ function will have to check for pinned items on the heap and skip over them. This
additional book-keeping is managable, When building the entirety of REDUCE, the number of pinned items is never
larger than 300. Considering memory used is in the order of megabytes, these pinned locations cause negligible
fragmentation.

** Variable lifetime
As the original language is decades old, its mechanism for variable lifecycle is not in line with that of modern languages.
This mechanism was counter-intuitive at first, and is lacking in providing safety to the user of the language.

There are two lifetime specifiers for symbols: /global/ and /fluid/. It is important to note that they
do not refer semantically to variables but only to *symbol names*.

A /global/ symbols has only a single globally visible value. That means you cannot bind the name to any local
variable. For instance if ~x~ is declared global, it then cannot be used as a function parameter name, or in a
let binding.

A /fluid/ variable has a global value, but can also be locally bound. Fluids behave more like globals do
in other languages, allowing the name to be reused.

~Let~ bindings and function parameters introduce /local/ symbols. If the symbol name is already declared global,
it will result in an error. If it is a fluid and has a global value, that value will be shadowed for the lifetime
of the binding.

To make it easier to use it as a scripting language, REDUCE's Lisp allows using symbols that aren't global, fluid,
or locally bound. These will act like local variables, except their lifetime will be defined for the duration of the
program. For single-threaded programs, this distinction is not important: these variables would act like fluids. However,
when implementing multithreading, fluids have a global value visible to all threads, while these symbols are only
visible on their local thread. I will call these symbols /unbound locals/.

** Saving state to disk
REDUCE has an important feature which allows the user to preserve the state to disk. A ~preserve~ instruction can be
used to do so, and the user is able to specify a function to run on restart. Preserve saves the entire state of the
program, including memory and symbols. The feature involves careful manipulation of the world state and it can maintaining
it unaffected when enabling multi-threaded programs is complicated. I ran into a few challanges when implementing ParVSL,
which I will discuss in the implementation chapter.

* Implementation
** Integrating threads
After familiarising myself with the VSL codebase I tried to implement the simplest form of
multi-threading and test out how the language would behave. I added a new function to VSL
which would takes a piece of code as an argument, starts a new thread and joins again with
the main thread. Once implemented, I could run first and simplest unit test for ParVSL:

#+BEGIN_SRC lisp
(dotimes (i 4) (thread '(print "Hello World")))
#+END_SRC
#+RESULTS:
: "Hello World"
: "Hello World"
: "Value: Hello Worln"Hello Woird"
: ld"

We can immediately observe the effects of parallelism in action. At this point, the code above
is just about the only working example. The interpreter is non thread-safe and data races
on global variables (including printing to the same stream) lead to undefined behaviour. Printing
still seems to work, however most other functions would fail. The spawned threads can only handle strings
and will crash on handling numbers or symbols, or when trying to allocate anything. 
There is no inter-thread communication, exception safety, and any garbage collection would produce a segmentation
fault. To be able to write a more complicated test, I need to make changes in all ares of the compiler.  

To manage the threads, I made futher use of the C++11 standard library. I created a global hashmap storing
all the running threads. Creating and joining a thread is done under a mutex lock. Each thread is assigned its own unique
identifier, which is returned to the user. The user can then use the identifier to join the thread.

** RAII classes
The RAII (Resource Allocation Is Initialisation) design pattern is common in C++. It is a programming tehnique
which binds the lifetime of resources to an object's lifetime. Normally,
C and C++ are manually managed, meaning all resources have to be carefully tracked by the programmer.
This makes it easy to introduce bugs when there is an attempted access to an unallocated resource,
or leaks when a resource is not released after use. C++ classes offer a solution to this problem.
Classes have both constructors and destructors, and these are automatically called when the object
is created and when it becomes unavailable, respectivelly.

#+BEGIN_SRC C++
{
  // obj1 is constructed here, unlike Java,
  // where it would be a null reference. 
  Foo obj1; 
  Bar obj2;
} // End of scope, obj2 is destructed, followed by obj1.
Foo *obj3 = new Foo(); // Foo constructor called here.
delete obj3; // Foo destructor is called here on obj3.
#+END_SRC

We can use this mechanism to implement automatic resource management, or RAII. We simply make sure
the underlying resource is allocated in the constructor and deallocated in the destructor.
Smart pointers like ~std::unique_ptr~ are a good showcase of the power of RAII. As soon as the
smart pointer objects become inaccessible (e.g by going out of scope), the underlying pointer
is safely deleted, providing a primitive (but very effective) form of automatic reference counting.

When chaning the codebase to use C++ features I found various opportunities to apply the RAII pattern,
when managing threads, shallow binding and garbage collection.

Thread objects in C++ are not implemented in a RAII style in the standard library. When a thread object
is destructed, it must be in an /unjoinable/ state. A thread is unjoinable if it either has joined or
it has been detached. If an unjoinable thread is ever destructed it will cause the entire C++ program to
terminate.

I have wrapped all threads in a ~ThreadRAII~ class, so that whenever they go out of scope they are automatically
joined (when still in a joinable state). This guarantess there will be no premature termination in the case the 
user does not handle the thread correctly and that the ParVSL interpreter will exit cleanly.

*** TODO wanted more robust code
modern tehniques
if there is an exception etc make sure it will always happen
previous code spent time testing error flags

** Storage management
All the memory is global and shared, and multiple threads will often try
to allocate concurently, causing contention. A naive solution to this problem
would have been to use a mutex lock on allocations. While this would be easy to
implement, it would also severely degrade performance. Locks are expensive even
in single-threaded scenarios with no contention, and allocations are very common
when evaluating Lisp. This is especially the case in an interpreted Lisp were no
allocations are optimisied away. Simply reading the code to evaluate allocates
\(O(n)\) times for code of length \(n\). That is because code in Lisp code is data.
Each instruction is a list data structure, with each elements allocated.
Serialising allocations is guaranteed to slow down the language enough to cancel 
any advantage multi-threading brings to the language.
 
*** Memory allocation
I wanted to allow multiple threads in parallel without affecting the performance
of allocations. To do this I had to use a lock-free approach. To do this, I further
split the memory into regions, which I call /segments/. A segment is a thread-local region
for allocation in memory.

Just like before, memory is allocated to the segments in a continuous 
fashion. A pointer indicates the start of the non-allocated part of the segment
(the /segment fringe/.), while another tells us the end of the segment
(the /segment limit/).

Now, contention is reduced to getting a new segment. Each thread only allocates within 
its own segment, so allocations do not require any synchronisation, and they still only
require incrementing one variable in most cases. If the requested allocation would bring
the fringe over the segment limit, then the current segment is /sealed/ and a new one is
requested. 

I carefully modified all the areas where allocations are preformed to use the segment fringe
instead of the global fringe. The global fringe is only moved to assign a new segment. Writes
to the global fringe are executed under a mutex lock, while the segment fringe is a thread local
variable accessed without any locks.

If the requested memory is larger than the segment size (e.g a large string or number), 
the current segment is sealed, then the large object will occupy its own custom segment, 
and the thread will then have to request a new segment.

There is a tradeoff involved when choosing the segment size. If the size is too small,
there will be a lot of contention on requesting segments, leading back to the original
problem of locking on every allocation. If the segment size is too large, there is a risk
of threads holding large amounts of memory without using it and causing early garbage collection
cycles. This is because reclaiming memory is requested when a new segment cannot be created,
regardless of how much free space there is in existing segments. While tradeoff depends on the
total memory, I have found a good compromise for segments of a few kilobytes each.

** Garbage collection
I will use the shortened form /GC/ to refer to garbage collection throughout the rest of this chapter for
brevity.

The garbage collector has to account for the state of all threads. These threads have to be synchronised
and in a safe state to initiate the GC. They must also be included in the calculation of the root set. 

I store all the threa-local information required for synchronisation in a class called ~Thread_data~. This
class is populated when a thread is started and updated before and after a GC cycle. All threads register
themseleves in a global thread table at startup. The thread starting the GC can use this table to check the status
of the other threads.

The /amabiguous root set/ is the set of potential references found on the stack. The stack is a continuous area
of memory to managed by incrementing and decrementing a stack pointer. The stack pointer is normally not accessible
from C++, however we can dereference a stack variable to find its location on the stack. VSL remembers the position
of the stack at the beginning of the execution and calculates this again before garbage collection. Then it
scans the all locations in between for ambiguous roots.

#+BEGIN_SRC C++
uintptr_t C_stackbase;

int main() {
  // t is any variable on the stack, before execution of VSL code begins
  int t; 

  // When starting VSL, we store the base of the stack
  // Here we need to cast the reference to an in type 
  // then align to the size of a LispObject
  C_stackbase = ((intptr_t)&t & -sizeof(LispObject));
}

void garbage_collection() {
  int t;
  // before GC, we note the head of the stack
  uintptr_t C_stackhead = ((uintptr_t)&t & -sizeof(LispObject));

  // scan the stack from its head to base (the stack grows downwards)
  for (uintptr_t s = C_stackhead;
       s < C_stackbase; 
       s += sizeof(LispObject)) 
  {
    if (in_heap(s)) { // check if s points within our virtual heap
      // we found an ambiguous root
      set_pinned(s);
    }
  }

  inner_garbage_collection();
}
#+END_SRC

Each thread will have its own stack, so I had to modify the code to scan all the stacks before garbage
collection. This is one reason I had to pause work on all threads for GC. If I hadn't, then a thread might
change add references to heap objects on its stack after those objects have been marked safe to delete.

When a thread is initialised, I save its own stackbase in ~Thread_data~, and then also save its stackhead
when it is paused to wait for GC. All these stack ranges are scanned before I start garbage collection:

#+BEGIN_SRC C++
void garbage_collection() {
  for (auto thread: thread_table) {
    // scan the stack from its head to base (the stack grows downwards)
    for (uintptr_t s = thread.C_stackhead; s < thread.C_stackbase; s += sizeof(LispObject)) {
      if (in_heap(s)) { // check if s points within our virtual heap
        // we found an ambiguous root
        set_pinned(s);
      }
    }
  }

  inner_garbage_collection();
}
#+END_SRC

*** TODO Garbage collection locks and safety
The garbage collector is /stop-the-world/. The thread initiating garbage collection must wait for all
threads to be ready. Similarly, all threads must regularly check whether a garbage collection cycle was
initiated and act accordingly.

The first idea I had was to trap all calls to allocate memory and check whether garbage collection is needed.
To do this, I could simply reset all thread segments. Threads would need to allocate eventually and
would request a new segment, at which point the would need to call the GC. This solution is incomplete however.
First of all, a thread might be busy for a long time without needing to allocate. This would cause all other
threads to be idle waiting for it to finish. 

A bigger issue was the risk of deadlocks. If thread A is waiting for the result of a computation on thread B,
but thread B was paused waiting for the GC, then the program is deadlocked. Similarly, any effectful computation,
like waiting for user input will prevent the collection from starting. 

#+BEGIN_SRC C++
// global variables for synchronising garbage collection
std::atomic_int num_threads(0);
std::atomic_int safe_threads(0);
std::condition_variable gc_waitall;
std::condition_variable gc_cv;
std::atomic_bool gc_on(false);
#+END_SRC

To deal with the issue of blocking calls, I defined another state threads can be in: /safe for GC/. A thread
is in a safe state if it has saved all the information the garbage collector needs to begin (e.g stackbase
and stackhead) and guarantees not to run any code that invalidates the garbage collection. Threads go into a safe
state whenever they get paused for GC. However, they can also be in safe state when waiting for a blocking call.

I created a special RAII class to handle both scenarios, which I called ~Gc_guard~. The class only has a constructor
and a destructor and is a way for the thread to promise it is in a safe state.

#+BEGIN_SRC C++
class Gc_guard {
public:
  Gc_guard() {
    int stack_var;
    // save the stackhead
    thread_data.C_stackhead =
      (LispObject *)((intptr_t)&stack_var & -sizeof(LispObject));
    paused_threads += 1; 

    // notify the gc thread that this thread is in a safe state
    gc_waitall.notify_one();
  }
   ~Gc_guard() {
    std::mutex m;
    std::unique_lock<std::mutex> lock(m);

    // wait here for gc to finish
    gc_cv.wait(lock, []() { return !gc_on; });
    paused_threads -= 1;
    
    // invalidate the stackhead
    thread_data.C_stackhead = nullptr;
  }
}
#+END_SRC

The ~Gc_guard~ class is accompanied by ~Gc_lock~. A thread trying to initiate the GC will have to
acquire a ~Gc_lock~. The lock will wait until all threads are in a safe state and will prevent
other threads from acquiring.

TODO: fix this code
#+BEGIN_SRC C++
class Gc_lock {
  std::mutex m;
  std::unique_lock<std::mutex> lock;
  Gc_guard gc_guard;
public:
  Gc_lock() : m(), lock(m) {
    int stack_var = 0;
    thread_data.C_stackhead =
      (LispObject *)((intptr_t)&stack_var & -sizeof(LispObject));

    paused_threads += 1;
    gc_waitall.wait(lock, []() { return paused_threads == num_threads; });
  }

  ~Gc_lock() {
    paused_threads -= 1;
    gc_on = false;
    thread_data.C_stackhead = nullptr;
    gc_cv.notify_all();
  }
};
#+END_SRC

*** TODO just having a non-stop gc would be its own project
reference java, etc. but what I have is good enough
interlocks add overhead, but w do not need real-time performance
not many threads

** Data races in the interpreter
The interpreter runs a read-eval-print loop which makes heavy use of global state and side effects.

All named objects in the lifecycle of the program are /symbols/. All global and local variables, including
special ones (like ~true~ or ~nil~) and function arguments are symbols. A global hash-table keeps track of
all symbols. This means each name can only be in use in one place at a time. 

Lisp is a language with dynamic scope. This has many implications for the interpreter. The following
fragment of code is an example of this behaviour:
#+BEGIN_SRC ocaml
let f x = x + y in
let g () =
  let y = 3 in
  f 2
in
g ()
#+END_SRC

The above example will not compile in any statically scoped languages such as OCaml or C++ 
because the variable ~y~ is not defined in its scope.
Most dynamic languages, even weakly typed ones, like Javascript or Python, will allow equivalent
code as valid, but will encounter a runtime error because ~y~ is not defined.
Lisp, and VSL in particular, has a mach looser concept of scope.
In the example above, ~y~ is defined before the call of ~f~ and will remain defined until the
call to ~g~ returns. 

Shallow binding is the mechanism by which this is achieved. Each symbol is mapped to a single value.
When a variable name is bound, e.g. as a function parameter name during a function call or through a
~let~ statement, its old value is stored by the interpreter and replaced with the new value. At the end of the binding,
the old value is restored. The main advantage of this method is performance. Old values can simply be saved
on the stack:
#+BEGIN_SRC ocaml
let varName = expr1 in expr2
#+END_SRC

can be implemented as:

#+BEGIN_SRC C++
void implement_let(string varName, LispObject expr1, LispObject expr2) {
  LispObject symbol = lookup_symbol(varName);
  LispObject oldVal = value(symbol); // store the old value
  value(symbol) = eval(expr1);       // replace with new value
  eval(expr2);                       // evaluate the rest of the code
  value(symbol) = oldVal;            // restore old value
}
#+END_SRC

This mechanism was not designed with concurrency in mind, and is not thread-safe. 
Many variable names are reused multiple times in a program, for example ~i, j, count~, etc.
Multiple threads binding the same variable will cause override each other's values.

I wanted to fix this while keeping the same dynamic scoping semantics for backward compatibility 
and not affecting performance. To do this, I have allocated thread-local storage for symbols.
Global symbols were unaffected, because rebinding them is illegal in the language. However,
for non-globals I used a thread-local array to store the real value, and had the global storage
location point to the array location. Each local symbol gets its own unique array index for the lifecycle
of the program. Then, each thread will reserve that location within its local array for the symbol.
Then, I created the function:

#+BEGIN_SRC C++
LispObject par_value(LispObject symbol) {
  LispObject val = value(symbol);        // use the original value function here
  if (is_global(symbol)) return val;     // global symbols remain unaffected  

  // ASSERT: val is a Location otherwise
  Location loc = value_to_location(val); // "extract" location, an integer type
  return local_value(loc);               // get the thread_local value at that location               
}
#+END_SRC

I carefully replaced all calls to ~value~ to ~par_value~. Now, multiple threads accessing the same symbol
can do so safely, as they will each access their own versions. This eliminates data races entirely.

*** TODO make diagram
 
** Shared memory and global variables
VSL has dynamic scoping and exhibits shallow binding. This means there is a global
storage mapping each symbol to exactly one value. A variable is defined as long 
as the symbol is assigned a value. The user can explicitly mark a symbol as global
or fluid. A global symbol only has a global value and cannot be locally bound.
A fluid symbol can have both a global value and a locally bound value. The way this
is accomplished is by saving the global value on every binding and then restoring it
as soon as the binding is over. 

This shallow binding approach is incompatible with a multi-threaded program: a symbol
could be locally bound to different values on different threads. One approach to solving the
issue is to use a deep-binding approach: passing around an associative list mapping the symbols
to values. The approach would add a penalty to performance, however it should be investigated
how significant the trade-off is. Instead, I have modified the code to try to keep the
shallow-binding. 

Global values work just as before and no modification is needed. For local and
fluids however, I added a thread-local storage array. The global storage now only
holds a pointer to the array location where the actual value is stored. This way, each thread can
hold its own version of the symbol and modify it safely. For fluids, there will still be a global
shared value. Since I already used the global storage for the pointer, I added one more global
array, where the pointer indicates the global value.

*** TODO more comments
#+BEGIN_SRC C++
std::atomic_int num_symbols(0);

thread_local std::vector<LispObject> fluid_locals;
std::vector<LispObject> fluid_globals; // the global values

LispObject& local_symbol(int loc) {
  if (num_symbols > (int)fluid_locals.size()) {
    fluid_locals.resize(num_symbols, undefined);
  }

  return fluid_locals[loc];
}

LispObject& symval(LispObject s) {
  assert(isSYMBOL(s));
  if (is_global(s)) {
    // This value is in the same location as in VSL on the heap.
    return global_value(s);
  }

  // If it is not a global value, then it is a location, represented as an integer.
  size_t loc = get_int(global_value(s));
  LispObject& res = local_symbol(loc);
  // Here I assume undefined is a sort of "reserved value", meaning it can only exist
  // when the object is not shallow_bound. This helps me distinguish between fluids that
  // are actually global and those that have been bound. 
  // When the local value is undefined, I refer to the global value.
  if (is_fluid(s) && res == undefined) {
      return fluid_globals[loc];
  }
  // THis is either local or locally bound fluid
  return res;
}
#+END_SRC
*** TODO draw diagram

*** Shallow binding

To implement shallow binding 

** Lock-free hashtable for symbol lookup
Just like allocations are a critical region of code in VSL, so are symbol lookups.
Every occurence of a symbol must be looked upin the symbol table. If the symbol does
not exist, it will be created. Multiple threads looking up symbols will cause contention.
If two threads try to allocate the same symbol name at the same time, they will invalidate
the table. 

#+BEGIN_SRC C++
LispObject symbols[SYMBOLS_SIZE];

LispObject lookup(std::string name) {
  size_t loc = hash(name) % SYMBOLS_SIZE;

  LispObject bucket = symbols[loc];

  while (bucket != nil) {
    LispObject s = head(bucket); // first list element
    
    if (symbol_name(s) == name) {
      // found the symbol
      return s;
    }

    bucket = tail(bucket); // rest of the list
  }

  LispObject s = allocate_symbol(name);
  symbols[loc] = cons(s, bucket);

  return s;
}

#+END_SRC

As before, the naive solution would be to implement a mutex lock on the entire 
lookup function.

To improve on that I tried to use a reader-writer lock. Reader-writer locks allow grant access 
to either a single writing thread, or all the reading threads. This would would allow multiple
threads to lookup symbols at the same time, however, as soon as one one thread has to create a
symbol, all th reader have to wait for it to finish.

Moreover, the lookup function is two-step: first it tries to find a symbol, then it creates one 
if it didn't find any. If two threads lookup the same symbol at the same time, it is possible for
both of them to end try to create it at the same time. The reader-writer lock would not prevent this!
It would serialise the writes, so it does prevent undefined behaviour in C++, however it does still
create the symbol twice. The pointer to the symbol that the first thread returned from the function 
would become invalid.

I have found a third approach, based on the Compare-and-swap (CAS) instruction which solves the issue
above, while also providing a lock-free implementation. The symbol lookup table is currently implemented
as static array of linked list.

Instead, I replaced it with:
#+BEGIN_SRC C++
std::atomic<LispObject> symbol_table[TABLE_SIZE];

LispObject lookup(std::string name) {
  size_t loc = hash(name) % SYMBOLS_SIZE;

  LispObject bucket = symbols[loc].load(std::memory_order_acquire);

  while (bucket != nil) {
    LispObject s = head(bucket); // first list element
    
    if (symbol_name(s) == name) {
      // found the symbol
      return s;
    }

    bucket = tail(bucket); // rest of the list
  }

  LispObject s = allocate_symbol(name);

  LispObject new_bucket = cons(s, bucket);

  while (!symbols[loc].compare_exchange_strong(bucket, new_bucket, std::memory_order_release)) {
    // search for stored value
    LispObject old_bucket = bucket;   
    bucket = symbols[loc].load(std::memory_order_acquire);

    for (LispObject s; s != old_bucket; s = tail(s)) {
      if (symbol_name(s) == name) {
        // Another name created the symbol. Use that.
        return s;
      }
    }

    // Make sure we don't discard new symbols inserted by other threads.
    new_bucket = cons(s, bucket);
  }

  return s;
}
#+END_SRC
** Threads

To start a thread, the user
simply needs to call the ~thread~ function. This function takes as arguments a function name and the argument list
and a unique thread id is returned. The function is applied to the arguments in a new thread. The return value
is stored until the user call ~thread_join~ on the thread id, when they can access the respective value. 

#+BEGIN_SRC ocaml
let add x y = x + y in 
let tid = thread add (2, 3) in
let result = thread_join tid in
print result
#+END_SRC 
#+RESULTS:
: 5

This allows for simple inter-thread communication, through function parameters and returns. Any function can
be run in parallel by simply applying thread on it. This task based approach makes it easy to modify
single-threaded algorithms to run in parallel.

My implementation minimises overhead. The list of arguments is managed on the heap which is visible by all
threads, so a simple pointer is enough to pass them. For returning values, I must keep track of all
unjoined threads and their return values. I do this with a simple hashtable, mapping thread ids to their
return values. When a thread returns from its function, I update the hashtable with the returned value.
When the user joins a thread, I look up the value in the table, return it, then erase the mapping. All threads
have unique ids for the lifetime of the program, so there will never be conflicts in the hashtable.
This hashtable has to be thread-safe, and I have implemented it as described in Lock-free hashtable.

I must also be careful to prevent the garbage collector from collecting these parameters and return values.
Starting a thread is GC-safe: garbage collection will not begin while a new thread is starting up. This ensures
that threads are always in a safe state and registered in the thread-table correctly during garbage collection.
At that point, function parameters are tracked just like regular variables so they will be safe from GC.
However, return values are stored past the lifetime of their respective threads. To deal with them, I add them
to the root set at the beginning of garbage collection. 

The handling of both of these root sets has to account for multiple running threads. All the new
thread-local storage was added to the unambiguous root set. Additionally, each thread is running its
own stack so all the stacks has to be accounted for as the ambiguous root set. The latter is more complicated.
All threads have to be paused before garbage collection can begin so that they do not interfere
with memory. A simple way to do this is to enable a global flag which all threads check on a regular basis.
However, if we are not careful, this can easily cause a deadlock (e.g: thread A waits for a signal from
thread B, but thread B is waiting for garbage collection). To solve such issues, I need to make two
changes. First, I must modify the functions in the interpreter to poll the global flag. Then I have
to make all waiting calls put a thread into a /safe/ state before sleeping, so that the garbage collector
can proceed with the thread. 
** TODO Saving state to disk and reloading

One important feature of the language is the ability to preserve the state of the world at any
time and save to disk. It is difficult to keep the same guarantees when multiple threads are running:
preserving when some of the threads are running a computation is tricky to define properly. To simplify
matter, I have decided that all threads have to be joined before preserving. This way, the state of
the world is consistent and relatively easy to restore. I have modified parts of the code to write
all the thread-local data back into global storage and then restore it when reloading. This way the same
file format is preserved, and I have not broken compatibility between single and multi-threaded images. 


** TODO talk about undefined behaviour in C++?

I must be careful to not collect the data during garbage
 collection, and I cover that in the [[Garbage collection]] section.

** Bugs in REDUCE


*** The symbol `x`
** Thread pool

Once I showed  that ParVSL could run both single-thread code (i.e build REDUCE) and pass some
simple tests for multi-threading, I was able to write more complex code using threads.

Spawning hardware threads directly to parallelise each task can be undesirable. The user has
to manage the lifecycle of each thread, making sure to join it and also to manage the number
of available threads on the current hardware directly. Failure to do so will quickly result in 
oversubscribtion of threads. Each thread object comes with its own overhead including a local
stack and operating system handle, 

One of the success criteria for the project was to implement a thread-pool. The thread pool
is the smallest example of code using all the multi-threading features, but also of
inter-thread communication and synchronisation. The thread pool is based on a simple
safe queue implemented with a linked list, a condition variable and a mutex. It spawns
the number of available hardware threads as workers which poll the queue for waiting
jobs. The waiting is done using a condition variable, to allow the thread to sleep rather than
busy spin.

ParVSL is not strongly typed and has no algebraic data types. Please note the use of a lists
to structure data.

** Computing Groebner bases in parallel

REDUCE comes with a large set of packages which cover different aspect of computer algebra.
Gröbner basis is a package particularly useful for algebraic geometry. Any set of multi-variate
polynomials has a Gröbner basis. The process is a generalisation of both Gaussian techniques for
solving a system of linear equations and computing the greatest common divisor of multiple
univariate polynomials.

The Gröbner basis of a linear system will form a matrix in reduced echelon form. For example,
a Gröbner basis of $\mathbb{F} = \{ \}

Let $\mathbb{C}[\mathbf{x}]$ denote the polynomial ring in $n$ variables $\mathbb{C}[x_1, \dots, x_n]$.
A subset $I \subset \mathbb{C[\mathbf{x}]}$ is an \ideal\ if it satisfies:
1. $0 \in I$.
2. If $a, b \in I$ then $a + b \in I$.
3. If $a \in I$ and $b \in \mathbb{C[\mathbf{x}]}$ then $a \cdot b \in \mathbb{C[\mathbf{x}]}$.



* Evaluation
** TODO Single-threaded building of Reduce
My project involved modifying a large body of code and it was almost guaranteed
that I would introduce bugs during development. Since the work involves a complex,
multi-paradigm programming language, it is not possible to guarantee to cover all
possible scenarios that might exhibit new bugs. Thankfully, the language and its
direct application are well intertwined, so there is a vey large coverage test already
available to demonstrate the most functionalities of the language: building REDUCE.

REDUCE consists of around 400 thousand lines of code, and since VSL was built to
run it, all functionality in VSL is being used in REDUCE. In addition to that,
REDUCE comes with a comprehensive suite of regression tests, which were written over the
years to detect bugs in new code. Finally, almost every library in REDUCE contains a
set of tests. These tests involve a large amount of heavy computation, stress testing
many different algorithms, using large amounts of memory and requiring multiple cycles
of garbage collection.

Between building REDUCE and passing its tests, I can be confident that ParVSL retains
backwards compatibility and catch development bugs. Any error while running the code,
or any difference in output between VSL is considered a bug. This approach helped me
find most of the bugs in my code. The disadvantage was that when I had to debug there
was too much code running and it was difficult to pinpoint to origin of the problem.

To aid with this, I had multiple stages to build. The first stage was just building
the core, while the second involved building the libraries. If an error showed up while
building a library, most other libraries could be skipped to help pinpoint the problem.
The problem was not completely fixed as just building the core involved running
a very large amount of code, and libraries are also significant in size.

Most of the bugs I could have potentially introduced were data corruption bugs.
I used asserts in various places to try to get the program to crash as soon as possible
and find the issue early. Sometimes I had to introduce a system of binary searching the
problem by trapping the program early and checking for the state of the computation.


In addition to helping me find issues in ParVSL, building REDUCE also provided for a
good performance benchmark. The building process simply runs ParVSL code. 

*** Coverage

** TODO Memory Allocation
Indeed, we can easily see the impact when trying to build reduce:

|   | VSL | MUTEX | SEGMENT |
|---+-----+-------+---------|
|   |     |       | 2m1s    |


|   | VSL   | MUTEX | LOCK-free |
|---+-------+-------+-----------|
|   | 1m55s | 2m10s | 2m4s     |

** Stress tests
** TODO Parallel implementation of common algorithms
*** FFT
*** 
** TODO Parallel building of Reduce

The entirety of REDUCE is made up of RLISP code, which VSL and ParVSL
can simply run to build it. The process can be separated into two steps:
building the core of REDUCE, and then building all the additional packages.
The core satisfies most of the dependencies any of the additional packages
need, and its build takes half the time of all the other. 


** TODO baby groebner base package
** TODO Thread-local performance on Windows

#+OPTIONS: :export none
#+PLOT: title:"Thread-local performance" ind:1 deps:(2 3) type:2d with:histograms set:"yrange [0:]" file:"tls-plot.png" :exports none
| OS     | non-thread-local | thread-local |
|--------+------------------+--------------|
| Cygwin |            1.985 |       51.234 |
| Linux  |            1.635 |        1.655 |

#+CAPTION: Thread-local performance
#+NAME:   fig:tls-plot.png
[[./tls-plot.png]]

*** TODO look at other thread-supporting lisps systems
Chez scheme?



* Conclusions

  

*** TODO Concurrency stuff
explain there was 
*** TODO have a hard part

*** TODO Talk about bugs in VSL
*** TODO enough of the system has to work just to evalute RLISP
*** TODO talk about planning
*** TODO describe how I could have debugged it well
*** TODO less code, pseudocode

starting from a root, transfer the data into a new space
when you finish scanning for new space.
if it has been moved already, nothing to do, if it hasn't move it. 
storage management bugs
introduce lisp earlier
use cormen pesudocode
mentioning implementing thread early

#+LATEX: \newpage

* Appendix

** Cheney's algorithm 

#+BEGIN_SRC C++
// LispObject is just a pointer type
typedef uintptr_t LispObject;

// LispObject is a pointer type
uintptr_t fringe1, limit1; // heap1, where all allocations happen 
uintptr_t fringe2, limit2; // heap2 used for copying GC

LispObject allocate(size_t size) {
  if (fringe1 + size > limit1) {
    collect();
  }

  if (fringe1 + size > limit1) {
     // We are out of memory. Try to increase memory
     // ...
  }

  uintptr_t result = fringe1;
  fringe1 += size;
  return result;
}

// Two helper functions are needed
LispObject copy(LispObject obj) {
  size_t len = size(obj);

  LispObject new_obj = static_cast<LispObject>(fringe2);
  fringe2 += len;
  return new_obj;
}

uintptr_t copycontent(LispObject obj) {
  for (auto ref&: forward_references(obj)) {
    ref = copy(ref);
  }

  return static_cast<uintptr_t>(obj) + size(obj);
}

void collect() {
  // First we copy over the root set, which includes symbols.
  for (LispObject& symbol: symbols_table) {
    symbol = copy(symbol);
  }

  uintptr_t s = heap2;
  while (fringe2 < fringe2) {
    s = copycontent(static_cast<LispObject>(s));
  }

  swap(fringe1, fringe2);
  swap(limit1, limit2);
}
#+END_SRC 

** Gc_guard and Gc_lock

** Lock free symbol lookup

[[https://math.berkeley.edu/~bernd/what-is.pdf]]
