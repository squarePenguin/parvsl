#+TITLE: Parallelism in Lisp
#+AUTHOR: Andrei Vlad Badelita

DISCLAIMER: This document will be used as a scratchpad for documenting
my work on the project. It will be quite erratic and disorganised.

* Progress

*** TODO Write proposal
*** TODO write first draft


A lisp implementation with multi-threading support. To be used
in REDUCE to implement parallel numerical algorithms.

<2018-09-30 Sun 12:47>

Looks like I am going to use VSL, the lightweight implementation.
It is lighter(4k lines of code) and interpreted, but also slower.
Still it allows benchmarks etc. 

It already provides a conservative GC. I will augment that to support
multiple-threads, using a stop-the-world approach. 

*** TODO talk about garbage collection
*** TODO talk about 
*** TODO talk about thread preemption
*** TODO Use RAII for exception handling tyding

** Dealing with fluid variables

Fluid variables are symbols which are reused for local scope.
Currently, the interpreter simply saves the old value, rebinds
it then evaluates the function. At the end it pop back the old value.
This scheme does not work for a multi-threaded compiler, as local bindings
on on thread should not interfere with the other threads.

The solution would be to instead have the local a thread local space for values.

All fluid and local symbols need to be stored in thread-local storage. However,
symbols are modified to fluid/global only by interpreted call.
Idea: Make fluid and global builtin functions so I can have more control.

However, if the user can modify these flags, then how do I get control over them?

*** DONE thread_local storage
     CLOSED: [2018-11-13 Tue 18:04]
*** DONE builtin global and fluid functions
     CLOSED: [2018-11-13 Tue 18:04]
*** TODO unglobal and unfluid
*** TODO RAII shallow binding
*** TODO modify interpreted and prof functions 
*** TODO make sure index values don't trip GC
*** TODO Make the new locations unambigous roots
*** TODO Load symbols when starting from image
*** TODO Save the state of the thread_local variables to the image
*** TODO Keep fluids in a global store, to load them on new threads
**** Might help that we should always join all threads before closing so only the start thread matters
**** However need to scan all symbols and put values back
**** But placeholder value on every thread(is undefined ok?)



** Dealing with multiple stacks

To run the conservative GC, we have a pointer to the beginning of the stack
and one recorded before collection.

This approach does not really work for multiple threads as each one has their own
stack. We could stop all wait for all the threads to stop doing work and report
their stack, but this could suffer from deadlock. We would want a safe way for threads
to report they are clean, but how do we tell their stack at any point?
C++ does not allow preemption so how do we terminate a really long computation, or 
even an infinite loop?

All threads need to be put in a nice waiting state for garbage collection to kick in.
We need to use a flag that tells all threads when they should stop working.
They can simply go call the gc to do so. Thus, when they allocate they will be dealt
with automatically. The ones that do not allocate, are either running some interpreted
code or one of a few builtin functions. We need to treat each builtin separately
and check the flag while running. 


Add entry of fluid and global.
Original one didn't matter.
Symbol enter hashtable when read, and never collected.
(Interesting to have weak symbol table)

lisp was restarting

comment about these lisps
treat value cell as ambiguous?
look for global in gc?
use fixnum?


implement harness to bisect


* Performance

#+BEGIN_QUOTE
However even when I build using optimization level -O3 the naive tests I have tried show parfastvsl to be slower than fastvsl by about a factor of 10, which seems utterly astonishing to me. When I had seen the slowdown on the "-O0 -g" version I had expected it to be down to you using more inline functions and the non-optimised compilation of those having huge overheads, but there is clearly more than that, and it deserves a bit of investigation and comment. You do not need any of Reduce to observe that - try
   ./parvsl
   (dotimes (i 100000))
   (time)
or some such.


Hmm time to profile with "-pg"?? Or to wonder exactly what overheads std::vector imposes, eg by using gdb to step through instruction by instruction .

Hmmm if I make vsl and then edit csl.cpp just to put thread_local ahead of a few of the variable declarations then under cygwin it slows down quite badly, and that is vsl not parvsl. I have not checked on Linux and not thought about ways to avoid the extra cost. But that can turn into another page of writeup for you!!!! 
#+END_QUOTE

I have tried running the optimised parvsl with ~make fastparvsl~, ~make fastparrcore~ etc and did not get those results.
SO far I have the following results:

Building rcore: same time around 1m30s-1m40s for both vsl and parvsl
BUilding reduce: ~8m50s on vsl vs ~10m10s on parvsl
The dotimes test: ~2000 for vsl vs ~4000 for parvsl
Running the alg test ~./test1.sh alg~: 0.22s on both
So far it is a bit slower but nothing crazy like 10x. I'll look into this further.
