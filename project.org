#+TITLE: Parallelism in Lisp
#+AUTHOR: Andrei Vlad Badelita

DISCLAIMER: This document will be used as a scratchpad for documenting
my work on the project. It will be quite erratic and disorganised.

* Progress

*** DONE Write proposal
    CLOSED: [2018-12-20 Thu 08:43]
*** TODO write first draft


A lisp implementation with multi-threading support. To be used
in REDUCE to implement parallel numerical algorithms.

<2018-09-30 Sun 12:47>

Looks like I am going to use VSL, the lightweight implementation.
It is lighter(4k lines of code) and interpreted, but also slower.
Still it allows benchmarks etc. 

It already provides a conservative GC. I will augment that to support
multiple-threads, using a stop-the-world approach. 

*** TODO talk about garbage collection
*** TODO talk about 
*** TODO talk about thread preemption
*** TODO Use RAII for exception handling tyding

** Dealing with fluid variables

Fluid variables are symbols which are reused for local scope.
Currently, the interpreter simply saves the old value, rebinds
it then evaluates the function. At the end it pop back the old value.
This scheme does not work for a multi-threaded compiler, as local bindings
on on thread should not interfere with the other threads.

The solution would be to instead have the local a thread local space for values.

All fluid and local symbols need to be stored in thread-local storage. However,
symbols are modified to fluid/global only by interpreted call.
Idea: Make fluid and global builtin functions so I can have more control.

However, if the user can modify these flags, then how do I get control over them?

Actually, do I care about fluid local variables? Preserve should only come
from a global scope right?

*** DONE thread_local storage
     CLOSED: [2018-11-13 Tue 18:04]
*** DONE builtin global and fluid functions
     CLOSED: [2018-11-13 Tue 18:04]
*** DONE unglobal and unfluid
    CLOSED: [2018-12-20 Thu 08:42]
*** DONE RAII shallow binding
    CLOSED: [2018-12-20 Thu 08:42]
*** DONE modify interpreted and prof functions 
    CLOSED: [2018-12-20 Thu 08:43]
*** DONE make sure index values don't trip GC
    CLOSED: [2018-12-20 Thu 08:43]
*** DONE Make the new locations unambigous roots
    CLOSED: [2018-12-20 Thu 08:43]
*** TODO Load symbols when starting from image
*** TODO Save the state of the thread_local variables to the image
*** DONE Keep fluids in a global store, to load them on new threads
    CLOSED: [2018-12-20 Thu 08:43]
**** Might help that we should always join all threads before closing so only the start thread matters
**** However need to scan all symbols and put values back
**** But placeholder value on every thread(is undefined ok?)

Need to be extra careful when reloading. How do we tell if a fluid was global or local?
We care about local values on the main thread.

Saving image and loading is problematic. Easiest is to put all the thread_local value back into
the symbol ~qvalue~ and restore it back on loading. 

If the value was already global, nothign needs to be done.
If it was local, we put it in global storage and just restore.
If it was fluid, need to store both its global and local values. Use a cons.
Problem: on loading they disappear :(.

Maybe try instead to store all values as cons, before last garbage collection.
(perhaps you can't allocate after last compaction).

Actually I cannot load the cons so soon. Need to fix symbols in two steps.
FIrst we relocate value then we move them.

** Dealing with multiple stacks

To run the conservative GC, we have a pointer to the beginning of the stack
and one recorded before collection.

This approach does not really work for multiple threads as each one has their own
stack. We could stop all wait for all the threads to stop doing work and report
their stack, but this could suffer from deadlock. We would want a safe way for threads
to report they are clean, but how do we tell their stack at any point?
C++ does not allow preemption so how do we terminate a really long computation, or 
even an infinite loop?

All threads need to be put in a nice waiting state for garbage collection to kick in.
We need to use a flag that tells all threads when they should stop working.
They can simply go call the gc to do so. Thus, when they allocate they will be dealt
with automatically. The ones that do not allocate, are either running some interpreted
code or one of a few builtin functions. We need to treat each builtin separately
and check the flag while running. 


Add entry of fluid and global.
Original one didn't matter.
Symbol enter hashtable when read, and never collected.
(Interesting to have weak symbol table)

lisp was restarting

comment about these lisps
treat value cell as ambiguous?
look for global in gc?
use fixnum?


implement harness to bisect


* Performance

#+BEGIN_QUOTE
However even when I build using optimization level -O3 the naive tests I have tried show parfastvsl to be slower than fastvsl by about a factor of 10, which seems utterly astonishing to me. When I had seen the slowdown on the "-O0 -g" version I had expected it to be down to you using more inline functions and the non-optimised compilation of those having huge overheads, but there is clearly more than that, and it deserves a bit of investigation and comment. You do not need any of Reduce to observe that - try
   ./parvsl
   (dotimes (i 100000))
   (time)
or some such.


Hmm time to profile with "-pg"?? Or to wonder exactly what overheads std::vector imposes, eg by using gdb to step through instruction by instruction .

Hmmm if I make vsl and then edit csl.cpp just to put thread_local ahead of a few of the variable declarations then under cygwin it slows down quite badly, and that is vsl not parvsl. I have not checked on Linux and not thought about ways to avoid the extra cost. But that can turn into another page of writeup for you!!!! 
#+END_QUOTE

I have tried running the optimised parvsl with ~make fastparvsl~, ~make fastparrcore~ etc and did not get those results.
SO far I have the following results:

Building rcore: same time around 1m30s-1m40s for both vsl and parvsl
BUilding reduce: ~8m50s on vsl vs ~10m10s on parvsl
The dotimes test: ~2000 for vsl vs ~4000 for parvsl
Running the alg test ~./test1.sh alg~: 0.22s on both
So far it is a bit slower but nothing crazy like 10x. I'll look into this further.

Looks like on cygwin it is much slower. THread_locals add huge overhead.


* Problem running threads

Loads of different errors.
~(dotimes (i 100) (thread '(dotimes (i 10) (add1 i))))~
breaks
macroexpanding dotimes and running that still fails:
returns 'g002' etc/ prints them. these are generated symbols and should not be printed.


even when having different names altoghether we still get a bug 
threads sometimes return the wrong value(g001 instead of nil)

work1/work2 should be some way of threadlocal 
look at prog/return/go
explode

To synchronise threads:
each thread has a gc_ready flag
there is a global atomic counter: num_gc_ready
first thread runs out of memory, calls reclaim
they 

* lookup
- TODO lookup function needs a lock
- add to dissertation
- assert my atomics are lock free AND write in the dissertation
- each bucket can be an atomic and  use CAS


talk about work1, work2 were not thread local
talk about x as a global variable
talk about bug which goes away when debugging(printing helps)

just fixed bug where threads would get confused.
turns out I was setting thread ids incorrectly. I thought I was incrementing an atomic
under mutex so safe, but assignment was on the new thread so could see an incorrect value.